{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leox48/Disinformation-Threat-Intelligence/blob/main/Tuplet_Extractor_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhEDYalWkv7b"
      },
      "source": [
        "## Step 1: Installazione dei packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VuenKPqoNfS",
        "outputId": "92ec7007-c0ea-4fef-85ec-5acc309c3bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python -V  #Python 3.10.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj8FZCXshH-Y",
        "outputId": "9d51b09a-4356-49b0-c20e-1c5f1cc43bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version # find the CUDA driver build above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-oKv5ARNT9a"
      },
      "source": [
        "`cu121`: CUDA 12.1 - `cu122`: CUDA 12.2 - `cu123`: CUDA 12.3 - `cu124`: CUDA 12.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X15zY5debUpP",
        "outputId": "6e1e6da2-779a-4834-be74-3a4b8ec237d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n",
            "Collecting llama-cpp-python\n",
            "  Downloading https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels/releases/download/wheels/llama_cpp_python-0.2.26%2Bcu122-cp310-cp310-manylinux_2_31_x86_64.whl (28.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.1/28.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "xgboost 2.1.1 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\n",
            "accelerate 0.32.1 requires numpy<2.0.0,>=1.17, but you have numpy 2.0.1 which is incompatible.\n",
            "albucore 0.0.13 requires numpy<2,>=1.24.4, but you have numpy 2.0.1 which is incompatible.\n",
            "arviz 0.18.0 requires numpy<2.0,>=1.23.0, but you have numpy 2.0.1 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.1 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.1 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.1 which is incompatible.\n",
            "ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.0.1 which is incompatible.\n",
            "pandas 2.1.4 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 2.0.1 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.1 which is incompatible.\n",
            "scikit-learn 1.3.2 requires numpy<2.0,>=1.17.3, but you have numpy 2.0.1 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.1 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.0.1 which is incompatible.\n",
            "transformers 4.42.4 requires numpy<2.0,>=1.17, but you have numpy 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.26+cu122 numpy-2.0.1 typing-extensions-4.12.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (781.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Collecting torchaudio==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.1+cu121\n",
            "    Uninstalling torchvision-0.18.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.3.1+cu121\n",
            "    Uninstalling torchaudio-2.3.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.3.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "accelerate 0.32.1 requires numpy<2.0.0,>=1.17, but you have numpy 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 torchaudio-2.3.0+cu121 torchvision-0.18.0+cu121 triton-2.3.0\n"
          ]
        }
      ],
      "source": [
        "# Install key libraries for LLM\n",
        "\n",
        "#Install llama-cpp-python with CUDA per utilizzare la GPU\n",
        "!set CMAKE_ARGS=-DGGML_CUDA=on\n",
        "!set FORCE_CMAKE=1\n",
        "\n",
        "#Install llama-cpp-python, cuda-enabled package\n",
        "!python -m pip install llama-cpp-python --prefer-binary --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122 --force-reinstall\n",
        "\n",
        "#Install pytorch-related, cuda-enabled package\n",
        "!pip install torch==2.3.0 torchvision torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efJ_g2etSvLD"
      },
      "source": [
        "## Step 2: Download del LLM da hugging face\n",
        "\n",
        "Use the \"hf_hub_download\" function to download models on huggingface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpXQGhHlij6q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Modello LLAMA"
      ],
      "metadata": {
        "id": "fE0-947olU53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "fe50d2e2c8804b39a7621c70e8e9f300",
            "34024d889142412bb0423edbe95485c8",
            "01d042efa4bb4275b044c101d641416c",
            "5bd60f27249540629759b87e9af26f18",
            "301ae1e92ed24dd4b1f8c6961e00c0ec",
            "3a5b2867feff496980da408158ec95dc",
            "2067567d3d914860b5ae4135939bc63d",
            "13cc8ff94eff4db48bf41c90e035f300",
            "5706239284534b289104b82246c1edf5",
            "d990da007ea34af1a53ececfbba4d1d3",
            "9bb8726294894cc38468132b7013832d"
          ]
        },
        "id": "WsYot3Rz1NVf",
        "outputId": "e62b4948-8eec-4800-b235-01f53f5e2c43"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Llama-3-8B-Instruct-v0.10.Q8_0.gguf:   0%|          | 0.00/8.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe50d2e2c8804b39a7621c70e8e9f300"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My model path: models/Llama-3-8B-Instruct-v0.10.Q8_0.gguf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Define the model name and file\n",
        "model_name = \"MaziyarPanahi/Llama-3-8B-Instruct-v0.10-GGUF\"\n",
        "\n",
        "#Quantizzazione 1\n",
        "#model_file = \"Llama-3-8B-Instruct-v0.10.Q4_K_M.gguf\" # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred\n",
        "\n",
        "#Quantizzazione 2\n",
        "# model_file = \"Llama-3-8B-Instruct-v0.10.Q5_K_M.gguf\"\n",
        "\n",
        "#Quantizzazione 3\n",
        "#model_file = \"Llama-3-8B-Instruct-v0.10.Q8_0.gguf\"\n",
        "\n",
        "\n",
        "# Download the model from Hugging Face Hub\n",
        "model_path = hf_hub_download(\n",
        "    model_name,\n",
        "    filename=model_file,\n",
        "    local_dir='models/',  # Download the model to the \"models\" folder\n",
        "    )\n",
        "#Copia il path nel prossimo comando\n",
        "print(\"My model path:\", model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del llm #Ripulisci la gpu dai modelli caricati"
      ],
      "metadata": {
        "id": "E0Xn9dRQMEvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFMXgvWzckbU"
      },
      "source": [
        "Note that BLAS = 1 means GPU is enabled:\n",
        "*   AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4GldNtm_s-P"
      },
      "source": [
        "## Step 3: Utilizzo del LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBMlNefW2qnq"
      },
      "source": [
        "####Import del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0_onscHeKt4",
        "outputId": "9b3ec268-dea7-4e9e-8a2b-501b604a509f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ID                                                URL  \\\n",
            "0   1  https://web.archive.org/web/20161011224653/htt...   \n",
            "1   2  https://web.archive.org/web/20161015002001/htt...   \n",
            "2   3  https://web.archive.org/web/20160928083422/htt...   \n",
            "\n",
            "                                              TITOLO               SOURCE  \\\n",
            "0  Pope Francis Shocks World, Endorses Donald Tru...  Ending the Fed News   \n",
            "1  IT’S OVER: Hillary’s ISIS Email Just Leaked & ...  Ending the Fed News   \n",
            "2  Just Read the Law: Hillary is Disqualified fro...  Ending the Fed News   \n",
            "\n",
            "                                               TESTO  \\\n",
            "0  VATICAN CITY – News outlets around the world a...   \n",
            "1  Today Wikileaks released what is, by far, the ...   \n",
            "2  We’re so used to complicated problems that we ...   \n",
            "\n",
            "                   CAMPAGNA THREAT ACTOR TIPO  \n",
            "0  Viral Fake Election News          NaN  WEB  \n",
            "1  Viral Fake Election News          NaN  WEB  \n",
            "2  Viral Fake Election News          NaN  WEB  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('/content/DisinformNews Dataset.xlsx')\n",
        "print(df.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBDLfK8g2tHr"
      },
      "source": [
        "**Separazione delle colonne del dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VncP0-XokqtB"
      },
      "outputs": [],
      "source": [
        "# URL , TITOLO, SOURCE, TESTO, CAMPAGNA , THREAT ACTOR, TIPO\n",
        "colonna_id= df['ID']\n",
        "colonna_url = df['URL']\n",
        "colonna_titolo = df['TITOLO']\n",
        "colonna_source = df['SOURCE']\n",
        "colonna_testo = df['TESTO']\n",
        "colonna_campagna= df['CAMPAGNA']\n",
        "colonna_threat_actor = df['THREAT ACTOR']\n",
        "colonna_tipo = df['TIPO']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIJAgUxHvyfj",
        "outputId": "1f06861f-7662-4c79-bd77-8238a36cff78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VATICAN CITY – News outlets around the world are reporting on the news that Pope Francis has made the unprecedented decision to endorse a US presidential candidate. His statement in support of Donald Trump was released from the Vatican this evening: “I have been hesitant to offer any kind of support for either candidate in the US presidential election but I now feel that to not voice my concern would be a dereliction of my duty as the Holy See. A strong and free America is vitally important in maintaining a strong and free world and in that sense what happens in American elections affects us all. The Rule of Law is the backbone of the American government as it is in any nation that strives for freedom and I now fear that the Rule of Law in America has been dealt a dangerous blow. The FBI, in refusing to recommend prosecution after admitting that the law had been broken on multiple occasions by Secretary Clinton, has exposed itself as corrupted by political forces that have become far too powerful.Though I don’t agree with Mr. Trump on some issues, I feel that voting against the powerful political forces that have corrupted the entire American federal government is the only option for a nation that desires a government that is truly for the people and by the people.  For this primary reason I ask, not as the Holy Father, but as a concerned citizen of the world that Americans vote for Donald Trump for President of the United States.” Sources within the Vatican reportedly were aware that the Pope had been discussing the possibility of voicing his concern in the US presidential election but apparently were completely unaware that he had made a decision on going forward with voicing this concern until his statement was released this evening from the Vatican. Stay tuned to WTOE 5 News for more on this breaking news.\n",
            "1841\n"
          ]
        }
      ],
      "source": [
        "print(colonna_testo[0])\n",
        "print(len(colonna_testo[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqeh1Aej6BO6"
      },
      "source": [
        "####Setup temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK8lgU85hM-A"
      },
      "outputs": [],
      "source": [
        "#torch.cuda.empty_cache() # Clear GPU cache"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Temperatura=0**"
      ],
      "metadata": {
        "id": "QwVgw5ueul6l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSyxPuAXgfut"
      },
      "outputs": [],
      "source": [
        "def model_run(user_input):\n",
        "    # Definisci la sequenza di stop\n",
        "    stop_sequence = [\"##END LIST##\",\"#END LIST#\",\"END LIST\"]  # Sostituisci con la sequenza desiderata\n",
        "\n",
        "    # Crea la richiesta di completamento\n",
        "    output = llm.create_chat_completion(\n",
        "        messages= [\n",
        "            {\"role\": \"system\", \"content\": \"You are a bot that extracts representative elements of its high-level objective from the text\"},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        stop=stop_sequence\n",
        "    )\n",
        "\n",
        "    return output['choices'][0]['message']['content']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Temperatura = 0.3**"
      ],
      "metadata": {
        "id": "x2nKlWPv0iI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_run(user_input):\n",
        "    # Definisci la sequenza di stop\n",
        "    stop_sequence = [\"##END LIST##\",\"#END LIST#\",\"END LIST\"]  # Sostituisci con la sequenza desiderata\n",
        "\n",
        "    # Crea la richiesta di completamento\n",
        "    output = llm.create_chat_completion(\n",
        "        messages= [\n",
        "            {\"role\": \"system\", \"content\": \"You are a bot that extracts representative elements of its high-level objective from the text\"},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "        top_p=0.95,\n",
        "        stop=stop_sequence\n",
        "    )\n",
        "\n",
        "    return output['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "6LE-psjH0k95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Temperatura = 0.6**"
      ],
      "metadata": {
        "id": "agYdcjGgAevP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_run(user_input):\n",
        "    # Definisci la sequenza di stop\n",
        "    stop_sequence = [\"##END LIST##\",\"#END LIST#\",\"END LIST\"]  # Sostituisci con la sequenza desiderata\n",
        "\n",
        "    # Crea la richiesta di completamento\n",
        "    output = llm.create_chat_completion(\n",
        "        messages= [\n",
        "            {\"role\": \"system\", \"content\": \"You are a bot that extracts representative elements of its high-level objective from the text\"},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        stop=stop_sequence\n",
        "    )\n",
        "\n",
        "    return output['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "5QP2s4AyAdbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Funzione di estrazione dal testo"
      ],
      "metadata": {
        "id": "NUeUls9XBS1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_tuples(text):\n",
        "    # Trova la parte del testo prima di \"##END LIST## (1)\"\n",
        "    before_note = text.split(\"##END LIST## (1)\")[0]\n",
        "\n",
        "    # Regex per trovare le tuple nel formato \"soggetto - verbo - oggetto\"\n",
        "    # Ignora il numero iniziale seguito da un punto o altri caratteri\n",
        "    pattern = r'\\d*\\.*\\s*(.*? - .*? - .*?)\\n'\n",
        "\n",
        "    # Trova tutte le tuple\n",
        "    matches = re.findall(pattern, before_note)\n",
        "\n",
        "    return matches"
      ],
      "metadata": {
        "id": "3EfB8cSiuzuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHfbvCSenpwi"
      },
      "source": [
        "####Debug singolo run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY3ks3DJejoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4e03af-9384-4e25-8374-8ca7271af046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>> </s>\n",
            "<s>[INST] <<SYS>> \n",
            "Extract the main actions and relationships between entities mentioned in the text. Represent these as subject-verb-object (SVO) tuples.\n",
            "\n",
            "1. Paul Horner - was paid - $3,500\n",
            "2. Paul Horner - answered - a Craigslist ad\n",
            "3. Paul Horner - interviewed - with them\n",
            "4. Paul Horner - got the part \n",
            "5. Paul Horner - received - an actual check from 'Women Are The Future'\n",
            "6. Paul Horner - was told - if anyone asked, he should talk about Bernie Sanders\n",
            "7. Tom Downey - knew - those weren’t real protesters\n",
            "8. Tom Downey - shouted - at them the whole time\n",
            "9. Tom Downey - got high-fives - from his fellow Trump supporters\n",
            "10. Paul Horner - saw most of the people - during the interview and training for the rally\n",
            "11. Paul Horner - learned - they only paid Latinos $500, Muslims $600, African Americans $750, and illegals received $300 across the board\n",
            "12. Sarah Bradley - said she would protest Trump for free \n",
            "13. Sarah Bradley - thinks Trump supporters are the last kind of people to donate socks to the homeless\n",
            "14. David Mikkelson - told ABC News that he approves of what a story like this is accomplishing\n",
            "15. David Mikkelson - makes lots of money when a story goes viral and Snopes debunks it \n",
            "16. David Mikkelson - said they copy and paste parts of the original article into theirs, write a couple sentences, and that’s it\n",
            "17. David Mikkelson - wants to be clear about their website not doing any journalism or creative work\n",
            "18. David Mikkelson - thinks someone should start a company that debunks Snopes’ debunkings \n",
            "19. Kim LaCapria and Jeff Zarronandia - accuse the site of being a clickbait fake news site \n",
            "20. ABC News - reached out to Hillary Clinton’s campaign for comment but did not receive a response\n",
            "\n",
            "</s>\n",
            "<s>[INST] <<SYS>> </s>  <s>[INST] <<SYS>> \n",
            "The extracted tuples represent main actions and relationships between entities in the text, which are mostly about individuals being paid or involved in protests, people's opinions on Trump supporters, and the workings of Snopes.com. The subject-verb-object (SVO) format helps to identify these key elements.\n",
            "\n",
            "Some interesting points from the text include:\n",
            "\n",
            "1. Paul Horner was paid $3,500 to protest at a Donald Trump rally.\n",
            "2. He believes the group he worked with might be affiliated with Hillary Clinton's campaign.\n",
            "3. Tom Downey, a Trump supporter, claims that protesters were too organized and smart not to be real, but rather paid individuals.\n",
            "4. Sarah Bradley, a spokeswoman for Sock It Forward, disagrees with paying people to protest and thinks it's hypocritical of Trump supporters who don't donate to the homeless.\n",
            "5. David Mikkelson, founder of Snopes.com, acknowledges that his website makes money by debunking viral stories but doesn’t do actual journalism or creative work.\n",
            "\n",
            "These points highlight the main actions and relationships between entities in the text, showcasing a mix of political opinions, protests, and the workings of online news sites like Snopes.com. </s>\n",
            "<s>[INST] <<SYS>>  </s> <s>[INST] <<SYS>> \n",
            "The extracted tuples represent key elements from the given text, which mainly revolve around individuals being paid or involved in protests, people's opinions on Trump supporters, and the workings of Snopes.com.\n"
          ]
        }
      ],
      "source": [
        "user_input = f'''Read the following text and identify all the tuples in the subject-verb-object form. The tuples should reflect the main actions and relationships between the entities mentioned in the text. Follow these steps:\n",
        "\n",
        "              Identify the subject of the action.\n",
        "              Identify the verb that describes the action or relationship.\n",
        "              Identify the object or destination of the action.\n",
        "              Return the tuples in this format: Subject - Verb - Object.\n",
        "\n",
        "\n",
        "              Example:\n",
        "              Text: 'John gave a book to Mary.'\n",
        "              Tuple: 'John - gave - a book to Mary'\n",
        "              ##END LIST##\n",
        "\n",
        "              Apply this method to the following text:, {colonna_testo[9]} '''\n",
        "\n",
        "\n",
        "# Invia il messaggio e stampa la risposta\n",
        "response = model_run(user_input)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuples = extract_tuples(response)\n",
        "print(tuples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9OCMcus9PTh",
        "outputId": "7448cc25-95a0-45df-ca83-11d5af89553f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"James Comey - became - a Clinton Foundation defense contractor's recipient of millions of dollars\", \"James Comey - served - on a Clinton Foundation corporate partner's board\", 'James Comey - had - a financial relationship with his brother Peter Comey', 'James Comey - promised - to recuse himself on all cases involving former employers', 'James Comey - earned - $6 million in one year from Lockheed Martin', 'Lockheed Martin - became - a Clinton Foundation donor that very year', 'James Comey - left - the Bush administration and went to Lockheed Martin', 'Lockheed Martin - admitted - to becoming a Clinton Global Initiative member in 2010', \"Lockheed Martin - won - 17 approvals for private contracts from Hillary Clinton's State Department in 2010\", 'HSBC Holdings - partnered - with Deutsche Bank through the Clinton Foundation', 'James Comey - became - a board member, director, and Financial System Vulnerabilities Committee member of HSBC Holdings', 'James Comey - was not questioned - about his relationship with Peter Comey in his confirmation hearing', 'Peter Comey - serves - as Senior Director of Real Estate Operations for the Americas at DLA Piper', 'DLA Piper - performed - an independent audit of the Clinton Foundation in November', 'Peter Comey - bought - a $950,000 house in June 2008', \"James Comey and his wife - granted - a mortgage on Peter Comey's house for $711,000 in January 2011\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Run Iterato"
      ],
      "metadata": {
        "id": "GskMXLc5BXtf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKtGNF3nRG4B",
        "outputId": "a58bc862-1f32-4c05-f1f8-9364a7013e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " >>>>\n",
            "\n",
            "The tuples of main actions and relationships between entities are:\n",
            "\n",
            "Pope Francis - endorsed - Donald Trump\n",
            "Pope Francis - voiced - concern\n",
            "Pope Francis - felt - a dereliction of duty\n",
            "Pope Francis - expressed - opinion on American elections\n",
            "Pope Francis - stated - importance of strong America for world\n",
            "Pope Francis - mentioned - Rule of Law in America\n",
            "Pope Francis - criticized - FBI's refusal to recommend prosecution\n",
            "Pope Francis - criticized - corruption in the government\n",
            "Pope Francis - asked - Americans to vote for Donald Trump\n",
            "\n",
            "\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>>\n",
            "\n",
            "\n",
            "Here's a list of tuples:\n",
            "\n",
            "Hillary - was the subject of - Trump's dirty talk video\n",
            "Trump - called - Hillary 'the founder' of ISIS\n",
            "media - ripped apart - Trump over his statement about Hillary being the 'founder' of ISIS\n",
            "Assange - promised - latest batch of leaks would lead to the indictment of Hillary\n",
            "Wikileaks - released - a batch of leaked emails\n",
            "Hillary - admitted in her own words - to funding and running ISIS\n",
            "John Podesta - was the recipient of - an email from 2014, which was released today\n",
            "media - has not reported on - the content of the latest Wikileaks release yet\n",
            "Wikileaks - has been accurate in their leaks for a decade without releasing any false information\n",
            "Saudi Arabia and Qatar - funded 20% of - Hillary's Presidential campaign\n",
            "Clinton Foundation - received funding from - Saudi Arabia, Qatar, and others who are also funding ISIS\n",
            "Hillary - was complicit in the funding and arming of ISIS by our allies, Saudi Arabia and Qatar\n",
            "United States - created, armed, and funded - the terrorists overthrowing Syria via its Terrorist State allies\n",
            "Russia and Iran - may be fighting against - ISIS (in a different perspective)\n",
            "US is fighting against - Russia, while at the same time arming ISIS\n",
            "Hillary's campaign and the Clinton Foundation - are funded by the same people who fund ISIS and kill innocents in the Middle East and America\n",
            "The author of this text - thinks Hillary must be sent to trial for crimes against humanity and Treason\n",
            "people - should use social media to spread the truth about Hillary before the presidential election on November 8th\n",
            "you (the reader) - know what to do (spread the information)… \n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " >>\n",
            "<</SYS>> Extract the tuples of subject-verb-object in the provided text\n",
            "The extracted tuples are:\n",
            "\n",
            "Hillary - broke - a law\n",
            "Hillary - run - for president\n",
            "someone - ask - if there is a law that Hillary has broken\n",
            "someone - answer - the laws broken by Hillary are innumerable\n",
            "someone - mention - a specific law\n",
            "FBI - forget - to visit the Cornell Law Library\n",
            "Michael Mukasey - tells - MSNBC\n",
            "Michael Mukasey - point - to one federal law, Title 18. Section 20 71\n",
            "Cornell Law Library - provide - information from United States Attorney General Michael Mukasey\n",
            "Hillary - use - a private email server\n",
            "we (laypersons) - figure out - the solution that was hidden in plain sight\n",
            "FBI - investigate - Hillary’s email server\n",
            "FBI - fail - to find the law mentioned by Michael Mukasey\n",
            "Hillary - hold - any federal office\n",
            "Michael Mukasey - say - Hillary's private email server disqualifies her from holding any federal office\n",
            "we (laypersons) - wonder - what the FBI was doing when investigating Hillary’s email server\n",
            "someone - terminate - her presidential campaign\n",
            "Hillary - be - disqualified from holding any office under the United States\n",
            "Hillary - have - a presidential campaign\n",
            "who - replace - her on the Democrat ticket\n",
            "\n",
            "\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>>\n",
            "You are a bot that extracts representative elements of its high-level objective from the text\n",
            "\n",
            "The main actions, relationships, and entities in the given text can be represented by the following tuples:\n",
            "\n",
            "Michael Brown - shot/killed - Susan Brown\n",
            "Michael Brown - set on fire - house\n",
            "Michael Brown - turned gun on himself/self-inflicted - bullet to head\n",
            "Susan Brown - died of - gunshot wound prior to the house fire\n",
            "Michael Brown - spared life of - Dixie (beagle)\n",
            "Michael Brown - dropped off at neighbor's - Dixie (beagle)\n",
            "Neighbors - saw/smoke coming from - Brown residence\n",
            "Neighbors - called 911 at - approximately 11:50 p.m.\n",
            "Police/Fire crews - arrived on scene at - minutes later after the call\n",
            "Police Chief Pat Frederick - stated Mrs. Brown’s death was caused by a gunshot wound prior to the house fire\n",
            "Police Chief Pat Frederick - stated Mr. Brown's single-bullet head wound appears to be self-inflicted\n",
            "Police/Investigators - believe - murder-suicide scenario\n",
            "FBI Director James Comey - refused to comment at this time but asked for privacy and prayer\n",
            "\n",
            "\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " >> </s>\n",
            "Here are the extracted tuples:\n",
            "\n",
            "James Comey - was - FBI Director\n",
            "James Comey - accepted - millions of dollars from a Clinton Foundation defense contractor\n",
            "James Comey - had - financial relationship with his brother Peter Comey\n",
            "Peter Comey - works - at the law firm that does the Clinton Foundation's taxes (DLA Piper)\n",
            "James Comey - promised - to recuse himself on all cases involving former employers\n",
            "James Comey - earned - $6 million in one year alone from Lockheed Martin\n",
            "Lockheed Martin - became - a Clinton Foundation donor\n",
            "James Comey - served as - deputy attorney general under John Ashcroft for two years of the Bush administration\n",
            "James Comey - left - the Bush administration and went directly to Lockheed Martin\n",
            "Lockheed Martin - won - 17 approvals for private contracts from the Hillary Clinton State Department in 20\n",
            "James Comey - became a board member, a director, and a Financial System Vulnerabilities Committee member of HSBC Holdings in 2013\n",
            "HSBC Holdings - partnered with - Deutsche Bank through the Clinton Foundation to “retrofit 1,500 to 2,500 housing units” in “New York City”\n",
            "Peter Comey - serves as - Senior Director of Real Estate Operations for the Americas for DLA Piper\n",
            "James Comey - was not questioned about his relationship with Peter Comey in his confirmation hearing\n",
            "DLA Piper - performed - an independent audit of the Clinton Foundation in November\n",
            "James Comey and his wife - granted a mortgage on Peter Comey's house for $711,000\n",
            "Peter Comey - bought - a $950,000 house in June 2008\n",
            "First Savings Mortgage Corporation - gave a mortgage to Peter Comey for $712,500\n",
            "James Comey - left Lockheed Martin in 2010\n",
            "Lockheed Martin - admitted to becoming a Clinton Global Initiative member in 20\n",
            "Bill Clinton - received - $250,000 from the American Chamber of Commerce in Egypt for a speech in 20\n",
            "<\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>>>\n",
            "Please extract representative tuples in the subject-verb-object form from the given text. The tuples should reflect main actions and relationships between entities mentioned in the text.\n",
            "\n",
            "Here are the extracted tuples:\n",
            "Amadh Abu Makmud Al-alwani - put up - a video\n",
            "the video - was taken down - by YouTube\n",
            "Al-alwani - threatened - those who would decide to vote for Donald Trump\n",
            "Al-alwani - called - them “infidels” and “goat f—-ers”\n",
            "Al-alwani - claimed - Hillary was a woman, a “two face devil”, had the “charm of a pig” and was “treacherous as the snake”\n",
            "Al-alwani - warned - all Muslims not to take his warnings lightly\n",
            "Al-alwani - warned - supporters of Trump would be severely punished on earth and in Jannah (afterlife)\n",
            "Al-alwani - called - Trump “a dog, he is the scum of the earth”\n",
            "Al-alwani - explained - it was a sin to put an inferior being such as a woman into a position of power\n",
            "Al-alwan<i></i>i - explained - Satan's plan was to divide the Muslims of America by putting a woman in power\n",
            "Al-alwani - said - it would be dirty and danis (filthy) to vote for Trump\n",
            "Amadh Abu Makmud Al-alwani - rose as - a prominent ISIS leader figure\n",
            "\n",
            "\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " >>\n",
            "Here is the list of extracted tuples:\n",
            "\n",
            "Hillary Clinton - told - an audience\n",
            "Hillary Clinton - spoke - at the “Builders and Innovators Summit”\n",
            "Hillary Clinton - took - questions from the audience\n",
            "Hillary Clinton - said - that they are “most often the people that look over the horizon”\n",
            "Hillary Clinton - opined - they would bring in an entirely different mindset and strategies\n",
            "Hillary Clinton - added - a particular one she would just love to see running for the presidency at some point in the future\n",
            "Hillary Clinton - thinks - Donald Trump would be very successful if he were to venture into politics in the future\n",
            "Hillary Clinton - argued - that businessmen “can’t be bought”\n",
            "Hillary Clinton - praised - his successful projects and business ventures\n",
            "Hillary Clinton - praised - creating thousands of jobs\n",
            "Hillary Clinton - praised - understanding of philanthropic and charitable side of things\n",
            "Hillary Clinton - thinks - he understands the philanthropic and charitable side of things quite well\n",
            "Hillary Clinton - continued - Donald Trump has single-handedly helped stop the recession\n",
            "Hillary Clinton - said - in my book that’s hard evidence of the man’s business prowess and integrity, as well\n",
            "Hillary Clinton - think - everyone would agree those are commendable character traits for anyone looking to tackle the most difficult problems and challenges in the world\n",
            "Hillary Clinton - concluded - Donald Trump is someone I could very easily see winning the support of the people\n",
            "Hillary Clinton - ominously - concluded in her 2013 speech\n",
            "\n",
            "\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>>>\n",
            "You are a bot that extracts representative elements of its high-level objective from the text\n",
            "<</SYS>> \n",
            "Extracting main actions, relationships, and entities:\n",
            "- Randall Prince - found - approximately one dozen black, sealed ballot boxes filled with thousands of Franklin County votes for Hillary Clinton and other Democrat candidates\n",
            "- Randall Prince - went - to investigate\n",
            "- Randall Prince - saw - several black boxes in an otherwise empty room\n",
            "- Election officials in Franklin County, Ohio - are - reportedly stumped over what Randall Prince found \n",
            "- Randall Prince - is - a Trump supporter\n",
            "- The Clinton campaign - likely goal was - to slip the fake ballot boxes in with the real ballot boxes when they went to official election judges on November 8th\n",
            "- Many national Democrat groups - have pulled funding from Ohio, redirecting it to places where they are doing better\n",
            "- Hillary Clinton - has spent less time and money in recent weeks in Ohio as it appeared Trump will carry the state\n",
            "- With this find, now it appears that Clinton and the Democrat Party planned on stealing the state on Election Day\n",
            "- Randall Prince - was - doing a routine check of his company's wiring and electrical systems\n",
            "\n",
            "\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>>\n",
            "\n",
            "Here is the list of tuples in subject-verb-object form:\n",
            "\n",
            "Barack Obama - has - sensationally told\n",
            "Barack Obama - will NOT - vacate\n",
            "Donald J. Trump - is - elected\n",
            "Barack Obama - claims - he is fully prepared\n",
            "Barack Obama - found - a little known loophole\n",
            "Barack Obama - believes - is in the best interests\n",
            "Barack Obama - must - do what he feel\n",
            "Barack Obama - will be - forced to take\n",
            "Barack Obama - explained - As president I must\n",
            "Barack Obama - said - I am not standing down\n",
            "Barack Obama - responded - I am prepared\n",
            "Barack Obama - confessed - I am willing to file a motion of no confidence\n",
            "Blitzer - asked - if he would remain in charge\n",
            "Obama - responded - Yes if necessary\n",
            "Obama - cannot - allow him into the chair\n",
            "Obama - suggested - will barricade himself and his family inside the White House\n",
            "Secret Service - are instructed by Obama - use full force in defending the White House\n",
            "Joe - expressed willingness to die multiple times \n",
            "Howard Stern - questioned Trump - Don’t worry we’ll get some of the second amendment people\n",
            "Trump - seemed - unconcerned\n",
            "\n",
            "\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>> \n",
            "<</SYS>> \n",
            "Now, extract representative tuples from the text and return them as per the instructions provided. \n",
            "\n",
            "Here are the extracted tuples in the subject-verb-object format:\n",
            "\n",
            "1. Paul Horner - was - paid\n",
            "2. Paul Horner - answered - a Craigslist ad\n",
            "3. Paul Horner - interviewed - with them\n",
            "4. Paul Horner - got - the part\n",
            "5. Paul Horner - received - a check\n",
            "6. Paul Horner - was told - if anyone asked, he should talk about Bernie Sanders\n",
            "7. Paul Horner - communicated - with people who had AOL email addresses\n",
            "8. Tom Downey - knew - there was something up \n",
            "9. Tom Downey - shouted - at the protesters\n",
            "10. Tom Downey - got - high-fives from fellow Trump supporters\n",
            "11. Horner - saw - most of the protesters during interview and training\n",
            "12. Horner - learned - some people were paid less based on race, gender, or nationality \n",
            "13. Sarah Bradley - said - she would protest for free\n",
            "14. Sarah Bradley - told - Trump is creating a place for hate-filled individuals to gather\n",
            "15. Paul Horner - was taught - chants to shout at the rally\n",
            "16. Paul Horner - was given - a shirt with an offensive message\n",
            "17. David Mikkelson - said - Snopes makes money when it debunks a story\n",
            "18. David Mikkelson - told - they don’t do real journalism, but copy-paste and claim to debunk \n",
            "19. David Mikkelson - laughed - about the potential for a site debunking Snopes' debunkings \n",
            "20. ABC News - reached out - to Hillary Clinton's campaign for comment \n",
            "\n",
            "\n",
            "-----------------------FINE--------------------------------------\n",
            "response run time is:  388.83690667152405\n",
            "                                                TUPLA  ID ARTICOLO  \\\n",
            "0              Pope Francis - endorsed - Donald Trump            1   \n",
            "1                     Pope Francis - voiced - concern            1   \n",
            "2         Pope Francis - felt - a dereliction of duty            1   \n",
            "3   Pope Francis - expressed - opinion on American...            1   \n",
            "4   Pope Francis - stated - importance of strong A...            1   \n",
            "5   Pope Francis - mentioned - Rule of Law in America            1   \n",
            "6   Pope Francis - criticized - FBI's refusal to r...            1   \n",
            "7   Pope Francis - criticized - corruption in the ...            1   \n",
            "8   Pope Francis - asked - Americans to vote for D...            1   \n",
            "9   Hillary - was the subject of - Trump's dirty t...            2   \n",
            "10     Trump - called - Hillary 'the founder' of ISIS            2   \n",
            "11  media - ripped apart - Trump over his statemen...            2   \n",
            "12  Assange - promised - latest batch of leaks wou...            2   \n",
            "13    Wikileaks - released - a batch of leaked emails            2   \n",
            "14  Hillary - admitted in her own words - to fundi...            2   \n",
            "15  John Podesta - was the recipient of - an email...            2   \n",
            "16  media - has not reported on - the content of t...            2   \n",
            "17  Saudi Arabia and Qatar - funded 20% of - Hilla...            2   \n",
            "18  Clinton Foundation - received funding from - S...            2   \n",
            "19  United States - created, armed, and funded - t...            2   \n",
            "20  Russia and Iran - may be fighting against - IS...            2   \n",
            "21                            Hillary - broke - a law            3   \n",
            "22                      Hillary - run - for president            3   \n",
            "23  someone - ask - if there is a law that Hillary...            3   \n",
            "24  someone - answer - the laws broken by Hillary ...            3   \n",
            "25                 someone - mention - a specific law            3   \n",
            "26    FBI - forget - to visit the Cornell Law Library            3   \n",
            "27                    Michael Mukasey - tells - MSNBC            3   \n",
            "28  Michael Mukasey - point - to one federal law, ...            3   \n",
            "29  Cornell Law Library - provide - information fr...            3   \n",
            "30             Hillary - use - a private email server            3   \n",
            "31  we (laypersons) - figure out - the solution th...            3   \n",
            "32         FBI - investigate - Hillary’s email server            3   \n",
            "33  FBI - fail - to find the law mentioned by Mich...            3   \n",
            "34                Hillary - hold - any federal office            3   \n",
            "35  Michael Mukasey - say - Hillary's private emai...            3   \n",
            "36  we (laypersons) - wonder - what the FBI was do...            3   \n",
            "37    someone - terminate - her presidential campaign            3   \n",
            "38  Hillary - be - disqualified from holding any o...            3   \n",
            "39           Hillary - have - a presidential campaign            3   \n",
            "40         who - replace - her on the Democrat ticket            3   \n",
            "41          Michael Brown - shot/killed - Susan Brown            4   \n",
            "42                Michael Brown - set on fire - house            4   \n",
            "43  Michael Brown - turned gun on himself/self-inf...            4   \n",
            "44  Susan Brown - died of - gunshot wound prior to...            4   \n",
            "45    Michael Brown - spared life of - Dixie (beagle)            4   \n",
            "46  Michael Brown - dropped off at neighbor's - Di...            4   \n",
            "47  Neighbors - saw/smoke coming from - Brown resi...            4   \n",
            "48  Neighbors - called 911 at - approximately 11:5...            4   \n",
            "49  Police/Fire crews - arrived on scene at - minu...            4   \n",
            "\n",
            "                    CAMPAGNA  \n",
            "0   Viral Fake Election News  \n",
            "1   Viral Fake Election News  \n",
            "2   Viral Fake Election News  \n",
            "3   Viral Fake Election News  \n",
            "4   Viral Fake Election News  \n",
            "5   Viral Fake Election News  \n",
            "6   Viral Fake Election News  \n",
            "7   Viral Fake Election News  \n",
            "8   Viral Fake Election News  \n",
            "9   Viral Fake Election News  \n",
            "10  Viral Fake Election News  \n",
            "11  Viral Fake Election News  \n",
            "12  Viral Fake Election News  \n",
            "13  Viral Fake Election News  \n",
            "14  Viral Fake Election News  \n",
            "15  Viral Fake Election News  \n",
            "16  Viral Fake Election News  \n",
            "17  Viral Fake Election News  \n",
            "18  Viral Fake Election News  \n",
            "19  Viral Fake Election News  \n",
            "20  Viral Fake Election News  \n",
            "21  Viral Fake Election News  \n",
            "22  Viral Fake Election News  \n",
            "23  Viral Fake Election News  \n",
            "24  Viral Fake Election News  \n",
            "25  Viral Fake Election News  \n",
            "26  Viral Fake Election News  \n",
            "27  Viral Fake Election News  \n",
            "28  Viral Fake Election News  \n",
            "29  Viral Fake Election News  \n",
            "30  Viral Fake Election News  \n",
            "31  Viral Fake Election News  \n",
            "32  Viral Fake Election News  \n",
            "33  Viral Fake Election News  \n",
            "34  Viral Fake Election News  \n",
            "35  Viral Fake Election News  \n",
            "36  Viral Fake Election News  \n",
            "37  Viral Fake Election News  \n",
            "38  Viral Fake Election News  \n",
            "39  Viral Fake Election News  \n",
            "40  Viral Fake Election News  \n",
            "41  Viral Fake Election News  \n",
            "42  Viral Fake Election News  \n",
            "43  Viral Fake Election News  \n",
            "44  Viral Fake Election News  \n",
            "45  Viral Fake Election News  \n",
            "46  Viral Fake Election News  \n",
            "47  Viral Fake Election News  \n",
            "48  Viral Fake Election News  \n",
            "49  Viral Fake Election News  \n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "#print(content)\n",
        "\n",
        "# Calculate runtime\n",
        "#runtime = end_time - start_time\n",
        "#print(\"response run time is: \", runtime)\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "for i in range(10):\n",
        "\n",
        "  torch.cuda.empty_cache() # Clear GPU cache\n",
        "  user_input = f'''Read the following text and identify all the tuples in the subject-verb-object form. The tuples should reflect the main actions and relationships between the entities mentioned in the text. Follow these steps:\n",
        "\n",
        "              Identify the subject of the action.\n",
        "              Identify the verb that describes the action or relationship.\n",
        "              Identify the object or destination of the action.\n",
        "              Return the tuples in this format: Subject - Verb - Object.\n",
        "              Print \"END LIST\" at the end of the list.\n",
        "\n",
        "              Example:\n",
        "              Text: 'John gave a book to Mary.'\n",
        "              Tuple: 'John - gave - a book to Mary'\n",
        "              ##END LIST##\n",
        "\n",
        "              Apply this method to the following text:, {colonna_testo[i]} '''\n",
        "\n",
        "\n",
        "  # Invia il messaggio e stampa la risposta\n",
        "  response = model_run(user_input)\n",
        "  print(response)\n",
        "  print(\"-----------------------FINE--------------------------------------\")\n",
        "  # Estrazione delle tuple complete\n",
        "  tuples = extract_tuples(response)\n",
        "\n",
        "  if i == 0:\n",
        "    # Creare un DataFrame\n",
        "    df_tuple = pd.DataFrame(tuples, columns=['TUPLA'])\n",
        "     # Aggiungere la colonna 'ID' con il valore di colonna_ID[0]\n",
        "    df_tuple['ID ARTICOLO'] = colonna_id[i]\n",
        "    df_tuple['CAMPAGNA'] = colonna_campagna[i]\n",
        "  else:\n",
        "    # Create temporary DataFrames for new data\n",
        "    df_temp = pd.DataFrame({'TUPLA': tuples, 'ID ARTICOLO': colonna_id[i], 'CAMPAGNA': colonna_campagna[i]})\n",
        "    # Concatenate the temporary DataFrame with the main DataFrame\n",
        "    df_tuple = pd.concat([df_tuple, df_temp], ignore_index=True)\n",
        "\n",
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate runtime\n",
        "runtime = end_time - start_time\n",
        "print(\"response run time is: \", runtime)\n",
        "print(df_tuple.head(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wS4JrU-RrtH",
        "outputId": "7ba298f6-b6e8-4c38-c716-69fbde7c6687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "388.83690667152405\n",
            "File Excel creato con successo!\n"
          ]
        }
      ],
      "source": [
        "# Salvare il DataFrame in un file Excel\n",
        "df_tuple.to_excel('TuplesExtracted_Q8_06TEMP.xlsx', index=False)\n",
        "print(runtime)\n",
        "print(\"File Excel creato con successo!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RT_Q5KM_0TEMP=371.50677728652954\n",
        "RT_Q5KM_03TEMP=385.1130666732788\n",
        "RT_Q5KM_06TEMP=394.9632875919342\n",
        "RT_Q8_0TEMP=340.24108028411865\n",
        "RT_Q8_03TEMP= 390.331839799881\n",
        "RT_Q8_06TEMP= 388.83690667152405"
      ],
      "metadata": {
        "id": "TZYg7oi2vPuU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fe50d2e2c8804b39a7621c70e8e9f300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34024d889142412bb0423edbe95485c8",
              "IPY_MODEL_01d042efa4bb4275b044c101d641416c",
              "IPY_MODEL_5bd60f27249540629759b87e9af26f18"
            ],
            "layout": "IPY_MODEL_301ae1e92ed24dd4b1f8c6961e00c0ec"
          }
        },
        "34024d889142412bb0423edbe95485c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a5b2867feff496980da408158ec95dc",
            "placeholder": "​",
            "style": "IPY_MODEL_2067567d3d914860b5ae4135939bc63d",
            "value": "Llama-3-8B-Instruct-v0.10.Q8_0.gguf: 100%"
          }
        },
        "01d042efa4bb4275b044c101d641416c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13cc8ff94eff4db48bf41c90e035f300",
            "max": 8540770944,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5706239284534b289104b82246c1edf5",
            "value": 8540770944
          }
        },
        "5bd60f27249540629759b87e9af26f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d990da007ea34af1a53ececfbba4d1d3",
            "placeholder": "​",
            "style": "IPY_MODEL_9bb8726294894cc38468132b7013832d",
            "value": " 8.54G/8.54G [03:17&lt;00:00, 31.5MB/s]"
          }
        },
        "301ae1e92ed24dd4b1f8c6961e00c0ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a5b2867feff496980da408158ec95dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2067567d3d914860b5ae4135939bc63d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13cc8ff94eff4db48bf41c90e035f300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5706239284534b289104b82246c1edf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d990da007ea34af1a53ececfbba4d1d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bb8726294894cc38468132b7013832d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}